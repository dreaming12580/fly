# model confg.


# not load data.
[gbdt]
shrinkage=0.1 #0.1
tree_num=1000
layer_num=5
thread_num=25
sample_feature=0.8
sample_instance=0.6
output_feature_weight=0
preprocess_maximum_memory=60
temp_dirname=gbdt_temp
load_cache=0
feature_mask=
save_model_epoch=100

# rate_adjust_method:
#   1. feature_decay (i, t) [default]
#       * rate = learn_rate / N(i, t)
#   2. decay (t) 
#       * rate = learn_rate / t 
#   3. constant.
#       * rate = learn_rate
# regularization_method
#   1. none:
#       no regularization.
#   2. l1:
#       L1-regularization.
#   3. l2:
#       L2-regularization.
[lr] # config for lr
learn_rate_adjust_method=feature_decay # {feature_decay, decay, constant}
iter_num=10  
regularization_method=none # {none, l1, l2}
regularization_weight=0.1
uniform_method=none # {none, pre, online}
learn_rate=0.001
min_loss=0.001
#min_loss_diff=1e-8
#momentum_ratio=0.1
#batch_update=1
#shrink_limit=12

[knn]
k=10

[meta]
class_num=10
meta_model=gbdt
meta_section=gbdt

[gbdt_test]
shrinkage=0.1
tree_num=1500
layer_num=5
thread_num=30

[mnn]
iter_num=500
learn_rate=0.1
min_loss=0.001
#final_loss_diff=1e-5
final_loss_diff=-1
layer_num=2
layer_width=10

[cglr] # config for lr
iter_num=100
learn_rate=0.005
min_loss=0.001
# if set >0, the iteration will end when diff delta is less than this config.
final_loss_diff=1e-6
#momentum_ratio=0.1
#batch_update=1

