# model confg.


# not load data.
[gbdt]
shrinkage=0.1 #0.1
tree_num=1000
layer_num=5
thread_num=25


[lr] # config for lr
# rate_adjust_method:
#   1. feature_decay (i, t) [default]
#       * rate = learn_rate / N(i, t)
#   2. decay (t) 
#       * rate = learn_rate / t 
#   3. constant.
#       * rate = learn_rate
#   4. shrink:
#       * if delta-loss < loss * cur_rate:
#           shrink: cur_rate *= 0.5
learn_rate_adjust_method=feature_decay
iter_num=10
learn_rate=0.001
min_loss=0.001
#min_loss_diff=1e-8
#momentum_ratio=0.1
#batch_update=1
#shrink_limit=12

[knn]
k=10

[meta]
class_num=10
meta_model=gbdt
meta_section=gbdt

[gbdt_test]
shrinkage=0.1
tree_num=1500
layer_num=5
thread_num=30

[mnn]
iter_num=500
learn_rate=0.1
min_loss=0.001
#final_loss_diff=1e-5
final_loss_diff=-1
layer_num=2
layer_width=10

[cglr] # config for lr
iter_num=100
learn_rate=0.005
min_loss=0.001
# if set >0, the iteration will end when diff delta is less than this config.
final_loss_diff=1e-6
#momentum_ratio=0.1
#batch_update=1

